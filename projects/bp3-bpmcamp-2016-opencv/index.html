<!DOCTYPE html>
<html class="no-js"> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"> <title>Automated Testing using Image Recognition &#8211; Arda C. Tugay</title> <meta name="description" content=""> <meta name="keywords" content="bp3, automation, automated, testing, brazos"> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="https://ardactugay.me/assets/img/logo.png"> <meta name="twitter:title" content="Automated Testing using Image Recognition"> <meta name="twitter:description" content="In 2016, I prototyped an application for automated testing using image recognition. This application was able to match screenshots of UI elements to their location on the screen and interact with these UI elements as well as finding the location of text on the screen, with the idea of allowing users to test without writing code."> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="Automated Testing using Image Recognition"> <meta property="og:description" content="In 2016, I prototyped an application for automated testing using image recognition. This application was able to match screenshots of UI elements to their location on the screen and interact with these UI elements as well as finding the location of text on the screen, with the idea of allowing users to test without writing code."> <meta property="og:url" content="https://ardactugay.me/projects/bp3-bpmcamp-2016-opencv/"> <meta property="og:site_name" content="Arda C. Tugay"> <meta property="og:image" content="https://ardactugay.me/assets/img/logo.png"> <!-- Webmaster Tools verfication --> <!-- Canonical --> <link rel="canonical" href="https://ardactugay.me/projects/bp3-bpmcamp-2016-opencv/"> <link href="https://ardactugay.me/feed.xml" type="application/atom+xml" rel="alternate" title="Arda C. Tugay Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link type="text/css" rel="stylesheet" href="/assets/main-bb2aeefacd336f222c3cd8db2c89d2d8ba3eb00b34476d7d082630538ee1fcdb.css"> <!-- JS --> <!-- Global Site Tag (gtag.js) - Google Analytics --> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-107528867-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments)} gtag('js', new Date()); gtag('config', 'UA-107528867-1'); </script> <script src="https://ardactugay.me/assets/js/bowser.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="https://ardactugay.me/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://ardactugay.me/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://ardactugay.me/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://ardactugay.me/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="https://ardactugay.me/favicon.png"/> <link rel="shortcut icon" href="https://ardactugay.me/favicon.ico"/> <!-- Background Image --> <!-- Post Feature Image --> </head> <body> <nav class="animated fadeIn"> <div class="logo-container"> <a href="https://ardactugay.me" class="logo"><img src="https://ardactugay.me/assets/img/logo.png" class="animated rotateIn"></a> <a href="https://ardactugay.me" class="logo-text"><span>Arda C. Tugay</span></a> </div> <div class="menu-icon"> <a> <i class="fa fa-bars fa-2x"></i> <i class="fa fa-times fa-2x" style="display: none;"></i> </a> </div> <div class="fluid-menu-container"> <div id="nav-item-about"><a href="https://ardactugay.me">About</a></div> <div id="nav-item-projects"><a href="https://ardactugay.me/projects/">Projects</a></div> <div id="nav-item-skills"><a href="https://ardactugay.me/skills/">Skills</a></div> </div> </nav> <!-- Header --> <div class="container-medium animated fadeIn project"> <div class="content"> <div class="post-title "> <div class="flex-header"> <div class="back-button"> <a href="https://ardactugay.me/projects/"> <i class="fa fa-chevron-left"></i> <span>Back to Projects</span> </a> </div> <div class="title"> <h1>Automated Testing using Image Recognition</h1> </div> <div class="info"></div> </div> </div> <div class="tech-tags"> <div class="tech-tag">OpenCV (3.x)</div> <div class="tech-tag">C++</div> <div class="tech-tag">Java 8</div> <div class="tech-tag">Java Native Interface (JNI)</div> <div class="tech-tag">Automated Testing Framework</div> </div> <!-- Content --> <h2 id="introduction">Introduction</h2> <p>At the yearly BPMCamp<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup> conference, developers were given the chance to show off ideas by presenting prototypes in a 2-hour open session. Having developed the <a href="https://ardactugay.me/projects/bp3-testing-framework">Automated Testing Framework</a> recently, I thought about a way to allow business users to design tests for BPM processes: They could write tests using what they interact with, meaning they could take screenshots of UI elements that need testing, fill in the parameters (i.e. tell the testing system how to interact with this element, such as clicking, entering a string, etc.), and let the testing system handle the rest of it. This would require the testing system to behave like how a human would when performing manual testing: Identify the element to test on the screen, interact with it, and finally verify that results reflect what was expected. I achieved this using image recognition algorithms.</p> <h2 id="preliminary-research">Preliminary Research</h2> <p>Preliminary research for this project revealed 3 related projects:</p> <ul> <li>Sikuli</li> <li>OpenCV</li> <li>Google Vision API</li> </ul> <h3 id="sikuli">Sikuli</h3> <p><a href="http://www.sikuli.org/">Sikuli</a> is a software package that automates “anything that’s seen on the screen” and comes in different packages. One of them is called <a href="http://sikulix.com/">SikuliX</a>, which is distributed as a standalone package that can be scripted using a variety of languages. It can also be downloaded and included in any JVM-based language as a dependency for greater control. However, having initially been developed as a research project, its Java API is not well documented.</p> <h3 id="opencv">OpenCV</h3> <p>A cross-platform computer vision library that has been around since 2000. Having used this library in college before, I was somewhat familiar with its image recognition capabilities. At the time of this project, this library was mature, even including a GPU interface for some of the image recognition capabilities. OpenCV comes with a Java wrapper; however, this wrapper is limited in functionality, as it does not provide an API for all OpenCV functionality, such as the GPU interface.</p> <h3 id="google-vision-api">Google Vision API</h3> <p>A comprehensive cloud offering by Google that performs various computer vision tasks, including optical character recognition (OCR). At the time of this project, the Google Vision platform was in its infancy and did not provide any examples of OCR performed on screenshot images.</p> <h2 id="design">Design</h2> <p>I decided to go with OpenCV as I had experience using it before, and it provided a decent GPU interface for its image recognition algorithms, which would increase performance. However, this GPU interface was not available through the OpenCV Java API, which meant I had to use C++. Despite the challenges, this was a great opportunity for me to get back into C++ and learn how to use the Java Native Interface (JNI).</p> <figure> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/arch.svg"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/arch.svg" style="max-width: unset; width: 25vw;" /></a> <center><figcaption>Architecture diagram showing the high level interaction between modules. Java calls to the C++ functions are made through JNI.</figcaption></center> </figure> <p>The first step was to implement the C++ applications - the element and text finders - which would be called by the Java application using JNI. Then, I had to extend the <a href="https://ardactugay.me/projects/bp3-testing-framework">Automated Testing Framework</a> (called “Application Executor” in the diagram above) to perform several simple tasks. The following had to be done, if the element finder was being used:</p> <ol> <li>Bring the app to be tested to the foreground.</li> <li>Take a screenshot.</li> <li>Send the screenshot and the image of the element to be found to the element finder, then start execution.</li> <li>Based on the response from the element finder, execute the test on the app in the foreground.</li> </ol> <p>If the text finder was being used:</p> <ol> <li>Bring the app to be tested to the foreground.</li> <li>Take a screenshot.</li> <li>Send the screenshot and text to be found to the text finder, then start execution.</li> <li>Based on the response from the text finder, execute the test on the app in the foreground.</li> </ol> <h3 id="element-finder">Element Finder</h3> <figure> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/element-finder-ssd.svg"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/element-finder-ssd.svg" style="max-width: unset;" /></a> <center><figcaption>System sequence diagram for the UI element finder, showing the interaction between different modules.</figcaption></center> </figure> <p>The element finder first checked to see if the image of the UI element was smaller than a threshold. If so, it enlarged both the screenshot and the element image. Bicubic interpolation worked best for preserving sharp edges of elements.</p> <figure> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/bicubic-vs-bilinear.png"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/bicubic-vs-bilinear.png" /></a> <center><figcaption>Bicubic (left) vs bilinear (right) interpolation of an input box element on the screen. Bicubic interpolation is better at preserving sharp edges</figcaption></center> </figure> <p>Next, the <a href="http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html">Speeded up robust features (SURF)</a> - an optimized version of Scale Invariant Feature Transform (SIFT) - algorithm was used to detect keypoints (i.e. areas of interest in the image) and descriptors for these keypoints (i.e. an internal representation that describes these keypoints, such as their gradient) in both the UI element image and the screenshot. These descriptors were then used to find the element in the screenshot. While there were other algorithms available for this process, such as FAST and ORB, SURF<sup id="fnref:4"><a href="#fn:4" class="footnote">2</a></sup> excelled at detecting images with blurring, which tends to happen with images that are scaled up.</p> <figure> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/surf-detection.png"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/surf-detection.png" /></a> <center><figcaption>Detection of keypoints and what the element matches in the screenshot (right).</figcaption></center> </figure> <p>Next, a brute force matcher was used to match the descriptors in the UI element image and screenshot, which went through all descriptors and found the best match. While the brute force matcher finds the best result, it can also take a long time to complete for larger datasets. In such cases, another matching algorithm called FLANN may be used. FLANN uses approximations to find a good match (which may not be the best match) and is reportedly more efficient on datasets that contain data on the order of thousands of entries<sup id="fnref:5"><a href="#fn:5" class="footnote">3</a></sup>.</p> <p>Finally, if a good number of matches were found (based on a preset threshold value), a perspective transform was applied on the element image, which allowed the proper screen coordinates to be obtained and sent back to the Java application, which interacted with the element on the screen.</p> <h3 id="text-finder">Text Finder</h3> <p>I suspected using OCR could be an issue when trying to detect text in a screenshot, since generally photos are much more detailed (200-300 ppi<sup id="fnref:2"><a href="#fn:2" class="footnote">4</a></sup>) than the text in a screenshot (72 ppi). Some research confirmed my suspicions and was proven by research done by others: <a href="https://ardactugay.me/assets/docs/projects/opencv-2016/screen-text-recognition-eval.pdf">An evaluation of HMM-based Techniques for the Recognition of Screen Rendered Text</a>.</p> <p>The research paper above showed that Hidden Markov Model (HMM) based algorithms fared better at detecting screen-rendered text as they didn’t need to segment characters for detection. This was important since segmentation of characters is difficult, due to anti-aliasing applied to screen-rendered text<sup id="fnref:3"><a href="#fn:3" class="footnote">5</a></sup>. OpenCV offered two options for OCR using HMM: <a href="http://docs.opencv.org/3.3.0/d7/ddc/classcv_1_1text_1_1OCRTesseract.html">OCRTesseract</a> (Google’s OCR library) and <a href="http://docs.opencv.org/3.3.0/d0/d74/classcv_1_1text_1_1OCRHMMDecoder.html">OCRHMMDecoder</a>. Both required training data, which was also <a href="https://github.com/opencv/opencv_contrib/tree/master/modules/text">provided by OpenCV</a>.</p> <figure> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/text-finder-ssd.svg"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/text-finder-ssd.svg" style="max-width: unset;" /></a> <center><figcaption>System sequence diagram for the text finder, showing the interaction between different modules.</figcaption></center> </figure> <p>Having decided to go with Tesseract OCR (due to its ease of use with OpenCV), the first step was to convert the screenshot into greyscale, then enlarge using Lanczos interpolation<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup> (as suggested by the research paper above), since Tesseract performed much better with higher resolution images. Other OCR algorithms also existed, such as a pure HMM-based solution or ABBYY; however, HMM-based solutions were difficult to set up and train, while ABBYY was a proprietary solution that was not provided for standalone use.</p> <figure> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/recognition-accuracies.png"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/recognition-accuracies.png" /></a> <center><figcaption>Tesseract performs much better with higher resolution images.</figcaption></center> </figure> <p>Next, the screenshot was binarized using the <a href="https://en.wikipedia.org/wiki/Otsu%27s_method">Otsu method</a>, which automatically determined the threshold by separating the image into its foreground and background pixels. Since binarized images consist only of black and white pixels, it helped reduce the anti-aliasing formed around the text during enlargement. In cases where there is white text on black background however, the image needs to be inverted to detect such text as well (which can be performed as a separate sequence of steps).</p> <figure class="half"> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/non-binarized.jpg"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/non-binarized.jpg" /></a> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/binarized.jpg"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/binarized.jpg" /></a> <center><figcaption>A regular image (left) and its binarized version (right)</figcaption></center> </figure> <p>Next, the binarized screenshot was dilated to increase the white region in the image (assuming white is the color of the foreground objects in the image), which allowed text to merge, making it easier to find the contours of the merged text.</p> <figure class="third"> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/normal.png"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/normal.png" /></a> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/dilated.png"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/dilated.png" /></a> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/contoured.png"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/contoured.png" /></a> <center><figcaption>The screenshot (left), then dilated (middle), and finally contoured (right). Clicking on the contour image will help with viewing the red lines of the contour.</figcaption></center> </figure> <p>Next, a bounding box was created around the contoured areas, allowing extraction of only the regions with text. This would later be given to Tesseract for analysis, after filtering out outliers (such as vertical text).</p> <figure> <a href="https://ardactugay.me/assets/img/projects/opencv-2016/regions.png"><img src="https://ardactugay.me/assets/img/projects/opencv-2016/regions.png" /></a> <center><figcaption>Red box outlining the area with text, found by drawing a bounding box around the contour.</figcaption></center> </figure> <p>Finally, the regions can be given to Tesseract for extracting text. If any matching text is found within these regions, the location is sent back to the Java application such that tests can interact with the app in the foreground.</p> <h2 id="challenges">Challenges</h2> <p>Despite being an experimental app, I faced several challenges in the few weeks that I spent on finding a solution:</p> <ul> <li>There were no examples on how to perform text extraction from screenshots. I had to rely on research papers and examples that were similar to what I was trying to achieve (such as extracting subtitles from video).</li> <li>Last time I had used C++ was more than a decade ago. Much had changed with regards to the language and I was not familiar with some of the new features that was brought in with C++11 (such as <code class="highlighter-rouge">auto</code> and auto pointers). In addition, I had to use the C++ implementation of OpenCV, as the Java wrapper did not provide access to SURF, Tesseract, or any of the GPU interfaces.</li> <li>Using the GPU interface to find elements on a screenshot (with the element finder) proved to be significantly faster; however, with limited memory on the graphics card, I had to often release memory space allocated to matrices storing uncompressed images. With enlarged color images taking up about 120 MBs<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup> each, my graphics card was regularly running out of available memory.</li> <li>JNI had a difficult syntax that takes time to get used to. However, it provided seamless integration with C++. Here’s an example that references a Java native method signature in the package <code class="highlighter-rouge">com.example.elementfinder</code>, in the class <code class="highlighter-rouge">ElementFinder</code>, with name <code class="highlighter-rouge">findElementInScreenshot</code>:</li> </ul> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">JNIEXPORT</span> <span class="n">jobject</span> <span class="n">JNICALL</span> <span class="n">Java_com_example_elementfinder_ElementFinder_findElementInScreenshot</span>
        <span class="p">(</span><span class="n">JNIEnv</span> <span class="o">*</span><span class="p">,</span> <span class="n">jobject</span><span class="p">,</span> <span class="n">jstring</span><span class="p">,</span> <span class="n">jstring</span><span class="p">);</span></code></pre></figure> <ul> <li>Building OpenCV from source takes some time to get right, as you need to make sure all dependencies are available to build the library without issues (such as installing CUDA drivers, find the JNI C++ interfaces, etc).</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Although this was a difficult project, it is one that I greatly enjoyed experimenting with. I have learned about a plethora of image and text recognition algorithms, as well as the steps taken in utilizing such algorithms.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes"> <ol> <li id="fn:1"> <p>BPMCamp is an annual conference held by BP3, where employees and customers are invited to watch presentations by BP3 employees and industry guests. BPM stands for “Business Process Management”.&nbsp;<a href="#fnref:1" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:4"> <p>A drawback of SURF is that <a href="http://www.vision.ee.ethz.ch/~surf/download.html">it’s patented and requires permission to use commercially</a>.&nbsp;<a href="#fnref:4" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:5"> <p>There is no official source that states what a “large dataset” entails. The “order of thousands” statement is based on other users’ observations of FLANN’s performance.&nbsp;<a href="#fnref:5" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:2"> <p>“ppi” stands for “<a href="https://en.wikipedia.org/wiki/Pixel_density#Scanners_and_cameras">Pixels per Inch</a>”. It’s a way of measuring the pixel density of a screen or image. Higher pixel densities mean the image contains greater detail per square inch of area.&nbsp;<a href="#fnref:2" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:3"> <p>Anti-aliasing occasionally causes text to touch each other, which makes segmentation difficult.&nbsp;<a href="#fnref:3" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:6"> <p>Similar to cubic interpolation. You can read more about it <a href="https://en.wikipedia.org/wiki/Lanczos_resampling">here</a>.&nbsp;<a href="#fnref:6" class="reversefootnote">&#8617;</a></p> </li> <li id="fn:7"> <p>A 10 megapixel, CV_32FC3 format image (3 channel, each 32-bit floating point values, or 96 bits per pixel) takes up about 995,328,000 bits or ~124.4 MBs.&nbsp;<a href="#fnref:7" class="reversefootnote">&#8617;</a></p> </li> </ol> </div> </div> <hr> <div class="back-button-end"> <a href="https://ardactugay.me/projects/"> <i class="fa fa-chevron-left"></i> <span>Back to Projects</span> </a> </div> <section id="disqus_thread" class="animated fadeInUp"></section> <!-- /#disqus_thread --> </div> <!-- JS --> <script type="text/javascript" src="/assets/custom/watch-hover-95c0d79c7a16c49e94e5f41e18af350ce334e7900af485c31573421246ca1d89.js"></script> <script src="https://ardactugay.me/assets/js/jquery-1.12.0.min.js"></script> <script src="https://ardactugay.me/assets/js/jquery.goup.min.js"></script> <script src="https://ardactugay.me/assets/js/jquery.magnific-popup.min.js"></script> <script src="https://ardactugay.me/assets/js/jquery.fitvid.min.js"></script> <script type="text/javascript" src="/assets/scripts-c0caa9dbd62ec1db9a7069b64d51c9cb55c498599f5033998c46909c9b05f7d3.js"></script> <!-- Nav --> <script type="text/javascript" src="/assets/custom/nav-0dd77c0caa30854a912511a3b9acdd86054a4be337a2bc9a96aeceb455ea317d.js"></script> <script type="text/javascript"> var disqus_shortname = 'ardactugay'; (function () { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a> </noscript> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
